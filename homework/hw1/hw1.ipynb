{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# MCIS6273 Data Mining (Prof. Maull) / Fall 2021 / HW1\n", "\n", "**This assignment is worth up to 20 POINTS to your grade total if you complete it on time.**\n", "\n", "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n", "|:---------------:|:--------:|:---------------:|\n", "| 20 | Monday, Sep 20 @ Midnight | _up to_ 20 hours |\n", "\n", "\n", "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n", "\n", "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n", "\n", "## OBJECTIVES\n", "* Explore the statistical properties of images and build a cloudiness detector\n", "\n", "* Gain more practice with the Exploratory Data Analysis (EDA) and statistical functions in Pandas using WWII enlistment data\n", "\n", "## WHAT TO TURN IN\n", "You are being encouraged to turn the assignment in using the provided\n", "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n", "`homework/hw1`.   Put all of your files in that directory. \n", "\n", "Then zip that directory,\n", "rename it with your name as the first part of the filename (e.g. `maull_hw1_files.tar.gz`), then\n", "download it to your local machine, then upload the `.tar.gz` to Blackboard.\n", "\n", "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n", "on the basics of using in Linux.\n", "\n", "If you choose not to use the provided notebook, you will still need to turn in a\n", "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n", "this homework.\n", "\n", "\n", "## ASSIGNMENT TASKS\n", "### (50%) Explore the statistical properties of images and build a cloudiness detector \n", "\n", "We take for granted our biological capabilities of vision \n", "    and perception.  Indeed, human vision, while not the most\n", "precise or even most capable of Earth species is quite good\n", "and when paired with the perceptual capabilities of our brain,\n", "is an incredible mechanism.\n", "\n", "The perception and understanding of the words on the screen or\n", "page you read this assignment right now, is indeed a feat of \n", "great coordination, between your eyes, brain and the connective\n", "tissues of the nervous system.\n", "\n", "Machine vision, on the other hand, is not an easy task.  Vigorous\n", "research over the last 40 years is now bearing fruit, upon which \n", "self-driving cars\n", "and intelligent object detectors are being integrated into \n", "our daily lives, in some cases when we are least aware of it and\n", "in others where we might not like them to be (i.e. facial recognition \n", "in retail contexts).\n", "\n", "As we move into statistical skill building in the data mining\n", "context, images offer a rich area to explore, and we will\n", "do so in this part of the assignment by building a rudimentary\n", "\"cloudiness detector\".\n", "\n", "You know that determining whether it is a cloudy, partly or sunny\n", "day is largely a trivial task, even for the young ones among us --\n", "most children can accurately determine whether it is sunny by age\n", "3, but this task, as you will see, may not be so easy for our \n", "digital machines.\n", "\n", "This part of the assignment will invoke a few new tools, \n", "\n", "* we will use [numpy](https://numpy.org/) to manipulate data arrays easily and transform them;\n", "* we will use [pandas](https://pandas.pydata.org/) to convert numpy data arrays back and forth and provide a richer representation of the data, as well as\n", "  invoke some of the built-in statistical and graphing tools;\n", "* we will use [matplotlib](https://matplotlib.org/) to manipulate images and image data.\n", "\n", "\n", "### Image representation\n", "As you likely already know images are represented in a computer\n", "as an $n \\times m$ array (matrix) consisting of a monochromatic, greyscale\n", "or color representation.  Each value in the array represents\n", "a _pixel_.  Consider the $10 \\times 10$ color RGB image, where\n", "each entry in the matrix is represented as a 3-tuple RGB\n", "value where each value in the tuple is in the range $0 - 255$. Here\n", "is a concrete example:\n", "\n", "$$\n", "I_{m,n} = \n", "\\begin{pmatrix}\n", "(215,191,136) & \\cdots & (232,94,254) \\\\\n", "(151,195,183) & \\cdots & (210,36,220) \\\\\n", "(141,225,155) & \\cdots & (48,31,65) \\\\\n", "\\vdots   & \\ddots & \\vdots  \\\\\n", "(210,23,125) & \\cdots & (151,54,128) \\\\\n", "(84,165,239) & \\cdots & (46,176,84) \\\\\n", "\\end{pmatrix}\n", "$$\n", "\n", "You will see this as an $n \\times m \\times 3$ array.\n", "Put a thumbtack in this representation as it will be used \n", "heavily in this part of the assignment.\n", "\n", "## Tools you'll need\n", "You will need to load and display images in this \n", "assignment.\n", "\n", "\n", "### `matplotlib.image.mpimg.imread()`\n", "To load an image, you can use the [`matplotlib.image.mpimg.imread()`](https://matplotlib.org/stable/api/image_api.html?highlight=imread#matplotlib.image.imread) function\n", "which will load the image into a numpy array as represented above.\n", "\n", "### `matplotlib.pyplot.imshow()`\n", "To display an image in your notebook, simply write:\n", "\n", "```\n", "# load an image using imread()\n", "rgb_img = matplotlib.image.mpimg.imread()\n", "\n", "# show the image in the notebook\n", "matplotlib.pyplot.imshow(rgb_img)\n", "```\n", "\n", "### `matplotlib.colors.rgb_to_hsv()`\n", "While working in RGB color space can be useful, there is another\n", "color space that affords us some advantages to quickly get at\n", "some of the image properties we're most interested in for\n", "analysis.  The [`matplotlib.colors.rgb_to_hsv()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.colors.rgb_to_hsv.html) function will\n", "convert our RGB image into an HSV equivalent.  HSV stands for\n", "Hue, Saturation, Value and it allows us to more easily identify\n", "colors and their intensities in an image.  You can read more \n", "about that in these links : \n", "\n", "* Stack Exchange: [Why do we use the HSV colour space so often in vision and image processing?](https://dsp.stackexchange.com/questions/2687/why-do-we-use-the-hsv-colour-space-so-often-in-vision-and-image-processing)\n", "* Hue, Value, Saturation at [leighcotnoir.com](http://learn.leighcotnoir.com/artspeak/elements-color/hue-value-saturation/)\n", "* What are Color Models? at [wigglepixel.nl](https://www.wigglepixel.nl/en/blog/what-are-color-models/)\n", "\n", "Here is an example:\n", "```\n", "rgb_img = matplotlib.image.mpimg.imread()\n", "\n", "# convert to an hsv representation\n", "rgb_hsv = matplotlib.colors.rgb_to_hsv(rgb_img / 255.)\n", "\n", "# important, don't forget the division by 255 to normalize all the values!!!\n", "\n", "```\n", "\n", "### `numpy.compress()` and `flatten()`\n", "In one part of the assignment you will be asked to convert\n", "the HSV image to a Pandas DataFrame.  Doing this can be\n", "done a number of ways, but one thing you will note is the\n", "array of the image is 3 dimensions -- that is it has\n", "a width, height and an HSV representation of the pixel\n", "which is itself a tuple of 3 values (H,S,V).\n", "\n", "See information about [`compress()`](https://numpy.org/doc/stable/reference/generated/numpy.compress.html),\n", "and information about [`flatten()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.flatten.html), which are \n", "just one way to help you get the array reformed to \n", "fit into the DataFrame in the tasks below.\n", "\n", "### `DataFrame.T`\n", "Transposing a DataFrame is equivalent to a vector transpose.\n", "\n", "Consider the column vector :\n", "\n", "$$\n", "V = \\begin{bmatrix}\n", "    1 \\\\\n", "    3 \\\\\n", "    6 \\\\\n", "    5 \\\\\n", "    9\n", "    \\end{bmatrix}\n", "$$        \n", "\n", "The transpose $V^T$ is a row vector:\n", "\n", "$$\n", "V^T = \\begin{bmatrix} \n", "        1 & 3 & 6 & 5 & 9 \n", "      \\end{bmatrix}\n", "$$ \n", "\n", "So if you have a DataFrame:\n", "```python\n", "df = pd.DataFrame([1, 3, 6, 5, 9], \n", "             columns=['values']\n", "```\n", "\n", "which gives:\n", "\n", "\\begin{center}\n", "\\begin{tabular}{lr}\n", "\\toprule\n", "{} &  values \\\\\n", "\\midrule\n", "0 &       1 \\\\\n", "1 &       3 \\\\\n", "2 &       6 \\\\\n", "3 &       5 \\\\\n", "4 &       9 \\\\\n", "\\bottomrule\n", "\\end{tabular}\n", "\\end{center}\n", "\n", "then the transpose\n", "```python\n", "df.T \n", "``` \n", "will give:\n", "\n", "\\begin{center}\n", "\\begin{tabular}{lrrrrr}\n", "\\toprule\n", "{} &  0 &  1 &  2 &  3 &  4 \\\\\n", "\\midrule\n", "values &  1 &  3 &  6 &  5 &  9 \\\\\n", "\\bottomrule\n", "\\end{tabular}\n", "\\end{center}\n", "\n", "## Na&iuml;ve Cloudiness Detection\n", "Building a real cloud object detector is out of the scope\n", "of this assignment, but we can actually build a pretty good one on\n", "statistically analyzing the image, and looking for the blue\n", "in the sky.  When you think of it, cloudiness is just a \n", "ratio of the blueness to non-blueness in the sky.  We might\n", "also need to add some caveats to this detector -- namely, it \n", "isn't very good at detection if there are other things in\n", "the image besides clouds (e.g. mountains, cars, etc.).  So\n", "we'll assume that the images we'll analyze with it will all be \n", "images of the sky (camera pointed up) and not images of lanscapes\n", "with sky or other types of objects with sky in them.\n", "\n", "Looking at it mathematically, if we count all the blue\n", "pixels in the image and assume the non-blue (non-sky) pixels\n", "are clouds, we might come rather close to accomplishing \n", "what we want.  So if $p_{all}$ are all the pixels in \n", "the image and $p_{blue}$ are all the blue pixels\n", "in our image then $p_{not\\_blue} = p_{all} - p_{not\\_blue}$, \n", "then cloudiness is given by\n", "\n", "$$\n", "C(p) = 1 - \\frac{p_{blue}}{p_{all}}\n", "$$\n", "\n", "that is to say, the cloudiness is what's left of the pixels which\n", "are not blue. This number will, of course, be between 0 and 1\n", "and thus can be interpreted as a percentage \"cloudiness\", where\n", "0 is a clear sky, and 1 is a fully cloudy sky.\n", "\n", "Using this simple insight will allow you to develop the detector. \n", "\n", "You may structure your code however you like, but one hint \n", "you might consider is to build a function that takes an \n", "image file name and performs the necessary transforms to \n", "return cloudiness.\n", "\n", "&#167;  Load image #1 (`img01.jpg`) into a variable and display it. Convert \n", "the image to HSV using the method described above and display the HSV version.\n", "\n", "\n", "&#167;  Using the HSV image data (converted from RGB), make a Pandas DataFrame\n", "which looks something like this when your done:\n", "    \n", "\\begin{center}\n", "\\begin{tabular}{lrrr}\n", "\\toprule\n", "{} &           H &         S &         V \\\\\n", "\\midrule\n", "0 &  205.479452 &  0.618644 &  0.925490 \\\\\n", "1 &  205.479452 &  0.613445 &  0.933333 \\\\\n", "2 &  205.479452 &  0.610879 &  0.937255 \\\\\n", "3 &  205.352113 &  0.617391 &  0.901961 \\\\\n", "4 &  205.957447 &  0.646789 &  0.854902 \\\\\n", "$\\cdots$ & $\\cdots$ &  $\\cdots$   & $\\cdots$ \\\\\n", "85905 &  206.582278 &  0.316000 &  0.980392 \\\\\n", "85906 &  206.582278 &  0.322449 &  0.960784 \\\\\n", "85907 &  204.761905 &  0.272727 &  0.905882 \\\\\n", "85908 &  204.761905 &  0.268085 &  0.921569 \\\\\n", "85909 &  204.761905 &  0.294393 &  0.839216 \\\\\n", "\\bottomrule\n", "\\end{tabular}\n", "\\end{center}\n", "\n", "\n", "NOTES: (a) Notice the H, S and V are the columnar values! (b) The H value may need to be rescaled by 360 if\n", "the native value coming from the transform has been normalized to between 0 and 1.  \n", "\n", "\n", "&#167;  Use the `DataFrame.hist()` method to produce a histogram\n", "of the H, S and V values for `img01.jpg`.  Make sure the histograms\n", "are in your notebook and answer the following questions:\n", "\n", "* What do you observe about the H values in the histogram?(HINT: you might want to look at the dominant color type in an HSV tool online)\n", "* What is interesting about the S values in contrast to H (ignore the axis scale in your answer)?\n", "\n", "\n", "&#167;  Use the `DataFrame.describe()` method to produce the\n", "descriptive statistics for the HSV data for `img07.jpg`.  \n", "Answer the following:\n", "\n", "* What is the mean and median H?\n", "* What about max and min for H?\n", "* What can you say about the standard deviation and how\n", "  it relates to the values between the 25% and 75%-tile?\n", "* Assuming a normal distribution, what is the expected\n", "  H range for the 1st standard deviation?\n", "\n", "\n", "&#167;  Use the `DataFrame.describe()` and `DataFrame.join()` to\n", "compare the descriptive statistics of `img01.jpg` and\n", "`img07.jpg` side by side in a single table.\n", "\n", "* What general observation can you make about these images?\n", "* When looking at the standard deviation and quartiles, \n", "  what would you say about cloudiness?\n", "\n", "\n", "&#167;  Write a function `pct_cloudy()` which takes three\n", "parameters `filename`, `h_range` and `s_range`.\n", "Where `h_range` and `s_range` take a tuple\n", "of with (min, max) which give the min and \n", "max range for the H and S parameters.\n", "\n", "A call might look like `pct_cloud(\"img01.jpg\", \n", "(200, 210), (.1, .2))`.\n", "\n", "Your function will return the percent cloudy\n", "as discussed above in the summary for this part.\n", "\n", "* Use the following values for `h_range` and `s_range` \n", "  and build a table with the percent cloudy for each **of the 10 \n", "  files in the `data/` folder** in the Github repo for this assignment.\n", "\n", "  ```python\n", "     h_range = (180,240)\n", "     s_range = (.25,1.0)\n", "  ```\n", "\n", "* Using the values given, do you feel the percent cloudiness\n", "  is active?\n", "* Explain why these values make sense (you will need to go\n", "  back to the HSV color wheel to answer this)?\n", "* Drawing from evidence in the sample files, give a concrete reason\n", "  why the statistical details of the sample files support\n", "  this range.            \n", "\n", "\n", "\n", "### (50%) Gain more practice with the Exploratory Data Analysis (EDA) and statistical functions in Pandas using WWII enlistment data \n", "\n", "No matter your position favorable, unfavorable or indifferent, the \n", "military generates a lot of data, most of which ordinary citizens\n", "will never see.  One especially interest data that is often released\n", "to the public are enlistment records, or the basic information about\n", "those who enlisted into armed services.\n", "\n", "The dataset we will explore in this part is from the 9 million\n", "or so records from World War II of the enlisted men and women\n", "of the US armed services from 1938 to 1946.  In these records\n", "are a treasure trove of information, including names, ages\n", "height, weight, race, marital status, education status\n", "and other vital information of the enlisted.  \n", "\n", "As a side note, this data was originally capture onto punch cards\n", "(yes, the same type of punch cards that were used to program \n", "the first digital computers) and subsequently converted to \n", "digital form and accessioned into the US National Archives.\n", "\n", "Some background on the data can be found from this source link:\n", "\n", "* Electronic Army Serial Number Merged File, ca. 1938 - 1946 [https://catalog.archives.gov/id/1263923](https://catalog.archives.gov/id/1263923)\n", "\n", "The file we will be working with is a fixed width file (FWF) meaning\n", "that the number of characters per line is the same and\n", "that ranges of columns indicate the data in the field.\n", "\n", "For example, if you look at page 44 in the [file](https://catalog.archives.gov/OpaAPI/media/1263923/content/arcmedia/electronic-records/rg-064/asnf/100.1ND_NC.pdf?download=false)\n", "you will notice that the layout of the file is given to you.  So \n", "for example, columns 9-32 are the full name of the enlisted\n", "while columns 67-68 give the enlisted's year of birth. You will realize\n", "this can be an efficient way to encode data when you have limited \n", "storage or memory resources, though the necessary mapping of fields\n", "to their meaning cannot be lost, or the file may be difficult (or impossible) to\n", "interpret later.  Luckily such mappings exist for this important\n", "dataset.\n", "\n", "An example of FWF data is given below (the first two rows were added to show the column numbers:\n", "\n", "```\n", "          1         2         3         4         5         6         7         8 \n", "012345678901234567890123456789012345678901234567890123456789012345678901234567890 \n", "17006058HANSON LUVERN J         770197745090840PVT 8FA 30 9   07718109996671451 02382.95 \n", "36427840MARCILLE JOSEPH C       611436167260942PVT 8BI 00 5   06102127368671357 10815.143 \n", "32853721LUSKY CHARLES R         230652390220343PVT 8NO 02 5   02324144331661277 05742.238 \n", "```\n", "\n", "For this assignment we're going to analyze some of the demographic data\n", "including age, race, marital status and weight data.  We \n", "will also take a look at the enlistment dates and see if\n", "the data match up with what was going on during that time in US history. \n", "\n", "I have prepared a random sample of the data (around 900K records)\n", "since the full 9M records will put undue stress on the Hub\n", "and such a sample is a good enough representation of the data. In doing\n", "so, I have removed missing lines from the file, but that\n", "is the extent of any filtering that was peformed.\n", "\n", "This file can be found on Github in the same folder\n", "as the [`hw1.ipynb`](https://github.com/kmsaumcis/mcis6273_f21_datamining/blob/main/homework/hw1/hw1.ipynb). You will use it to answer all questions\n", "in this part of the assignment.\n", "\n", "## Tools you'll need\n", "You will need to load the fixed width file (FWF) in this \n", "assignment.\n", "\n", "### `pandas.read_fwf()`\n", "Use the [`read_fwf()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html) method to load the fixed width file.  I have \n", "made a method for doing this which reduces the amount\n", "of time for you to figure out which columns apply\n", "to which fields -- this required me to use the \n", "reference documents from the 1940s, which was tedious,\n", "but thankfully such documentation existed even if \n", "it was a scan of a typewritten source!\n", "\n", "### `Series.value_counts()` and `Series.sort_values()`\n", "Both of these will be useful in getting the values\n", "sorted for the questions below. Study both of these methods carefully.\n", "\n", "### Data Cleaning Hints\n", "The data set you will be using is not perfect as it was machine\n", "translated and there are known issues in it.  For example, \n", "you may get information that may be out of specification,\n", "for example, height is in inches, but if you look into it,\n", "height data does have errors, as does weight (i.e. weights \n", "less than 100, heights greater than 90). Just remember data cleaning\n", "is an important necessary step before proceeding.\n", "\n", "&#167;  On September 16, 1940, The Selective Training and \n", "Service Act of 1940 was signed into law by President \n", "Franklin D. Roosevelt, which was the country's first peacetime \n", "draft designed to conscript troops in the event of war.\n", "The timing of the Act was unique, since the US did not\n", "enter World War II (WWII) until December 1941, but it \n", "nonetheless required\n", "all male US citizens ages 21 to 35 to register for the draft\n", "from which names were drawn through a lottery system.  Those\n", "called to duty were to serve in the military\n", "for one year.  From 1940 to 1947 nearly 10.1 million men and women\n", "were inducted under the Act, the majority of which \n", "served in WWII stateside or abroad.ASDAS\n", "\n", "To warm up, we will take the enlistment column `enlistment_date`\n", "and plot the data from our sample file.  \n", "\n", "* Generate a bar plot of the enlistment numbers from 1939 to 1946. \n", "  The $x$-axis will contain the year (in ascending order) and the $y$-axis the \n", "  number enlisted.\n", "\n", "* Does the sample data support the claim that the Act increased enlistment\n", "  during WWII?\n", "\n", "* What was the peak year of enlistment?  Is this supported by the\n", "  entry of the US into WWII in December 1941.  Explain why or why not.\n", "\n", "* What is the median age of those enlisted?  You may need to clean\n", "  the data since there are unusual `birth_year` values that must be removed.\n", "\n", "\n", "&#167;  During WWII large numbers of African-Americans entered the \n", "armed services, eventhough the US was still largely segregated\n", "and the armed services maintained separate forces until after\n", "WWII.  \n", "\n", "* What was the percentage of African-Americans enlisted in 1941, 1942\n", "  and 1943?\n", "\n", "* The percentage of African Americans in \n", "  the general population was 9.8% in the 1940 census. \n", "  Compare that with the percentage enlisted \n", "  from 1941-43.  How do these percentages compare?\n", "\n", "* Where were the top 5 states of residence (`res_state`) that African-Americans\n", "  enlisted from?  You will need to look at this file: [https://catalog.archives.gov/OpaAPI/media/1263923/content/arcmedia/electronic-records/rg-064/asnf/100.1CL_SD.pdf?download=false](https://catalog.archives.gov/OpaAPI/media/1263923/content/arcmedia/electronic-records/rg-064/asnf/100.1CL_SD.pdf?download=false) and on page 3\n", "  reference the state codes to determine the states.  You might first\n", "  want to group the state codes first, then sort, take the top 5, \n", "  then match the code to the state -- this method will\n", "  certainly save time.\n", "\n", "\n", "&#167;  Age and marital status are vital bits of information which \n", "are tracked elsewhere (e.g. the Census), but the WWII military\n", "enlistment data provides an ample (and unique) subset of the \n", "population to determine marital and age characterstics of \n", "US citizens.  It should be noted that this data does include\n", "women, since for the first time in US military history, women\n", "served in an offical capacity with their own\n", "branches of service: Women's Army Auxiliary Corps (WAC), Women\n", "Airforce Service Pilots (WASP) and the Women Accepted for \n", "Volunteer Emergency Services (WAVES).\n", "\n", "For the sake of trying to understand marital status, we\n", "will use the `component` field to restrict to _men_ and\n", "_women's_ marital status.  When the `component` is 7 \n", "it refers to enlisted men.  You can find the complete reference\n", "for these values on page 306 in [this document](https://catalog.archives.gov/OpaAPI/media/1263923/content/arcmedia/electronic-records/rg-064/asnf/100.1CL_SD.pdf?download=false).\n", "You will want to use `branch_alpha==\"WAC\"` to filter for women, indicating\n", "the Women's Army Auxiliary Corps.\n", "\n", "Use the data to answer the following questions:\n", "\n", "* What percentage of the enlisted where older than 30?  You may\n", "  need to filter the data to eliminate spurious data -- there are\n", "  some values which are not correct!\n", "* What are the percentages of single (without depedents) and married men enlisted?  The `marital_status` field will be `6` for \n", "  _single (without dependents)_ and `1` for _married_. \n", "* What is the median age of a single man?\n", "* What are the percentages of married women in the WAC?  \n", "  \n", "\n", "\n", "&#167;  Education of service personnel varied quite a bit, and some\n", "claim that less educated people are more eager to accept\n", "entry into the service, especially when skilled labor \n", "is associated with education level.  The draft was presumed\n", "to be random -- no one was to receive a preference into the service.\n", "Furthermore, more education is often associated with a \n", "deeper understanding of the social, economic and political impacts\n", "of war, and often those with college and graduate degrees would refuse \n", "to enter the service as _conscientious objectors_.\n", "\n", "Interestingly, the Selective Service Act of 1940 did provision for\n", "_conscientious objectors_ -- those who for religious or\n", "philosophical reasons did not want to take up arms and \n", "be confronted with having to kill another human being.  Such \n", "people were still\n", "forced into service (or jailed if they refused), but \n", "they were given non-combat duty\n", "and often were not eligible for veterans benefits once discharged.\n", "\n", "In the 1940 census the percent of the population age 25 and older\n", "with [a college degree (or higher)](https://www.census.gov/data/tables/time-series/demo/educational-attainment/cps-historical-time-series.html) was around 4.6% for all\n", "races and all genders.  The GI Bill was a benefit given to veterans\n", "to encourage [pursuing and competing a college degree](https://clear.dol.gov/Study/Going-war-and-going-college-Did-World-War-II-and-GI-Bill-increase-educational-attainment-0),\n", "which contributed to the general rise of college degrees through the 50s and 60s.\n", "\n", "\n", "\n", "We will answer the following questions using the educational\n", "attainment data in the `education` field of the data. \n", "\n", "The information on which fields map to which education level\n", "are on page 305 of [this document](https://catalog.archives.gov/OpaAPI/media/1263923/content/arcmedia/electronic-records/rg-064/asnf/100.1CL_SD.pdf?download=false).\n", "HINT: 4 years of college is code `8` and 4 years of high school \n", "(grade 9 through 12) is code `4`.\n", "\n", "* What percentage of the enlisted people 25 or older in this data\n", "  held college degrees (4 years of college)?\n", "\n", "* How does that compare to the national average from the Census\n", "  data discussed above?  Does this support the claim that the \n", "  draft was disproportionated favorable to the college educated \n", "  during WWII? Why or why not? \n", "\n", "* What percentage only had grammar school (code `0`) education -- you \n", "  can use all ages?\n", "\n", "\n", "&#167;  These last questions will deal with perhaps the dirtiest part of the dataset and will truly be exploratory, but we may be able to get at some interesting relationships while reserving strong judgement.\n", "Most of the people enlisted represented average healthy adults in the general population, but also those enlisted must adhere to basic physical standards as set by the service. This remains true today (though the standards have changed over time) since basic physical conditioning and evaluation is required to enter the service, so that serious medical conditions do not present issues for basic perfomance of combat duties.  We are going to find out what the weight characteristics are of those entering the service in WWII.\n", "\n", "* Clean the data, and restrict values only to those that make sense -- for example, no one born before 1890 (age 50) and born after 1923 (age 18).\n", "* What is the median weight of those age 19-23?  Compare the median weight in 19-23 to the mean for the same age range. What are the differences? How does the standard deviation help interpret answer?\n", "* Plot a line plot of age to median weight and weight standard deviation. Age will be on the $x$-axis, weight on the left $y$-axis and weight standard deviation on the right $y$-axis? Your plot will have two lines -- one with the standard deviation, the other with the weight.\n", "\n", "\n"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [default]", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "12px", "width": "252px"}, "navigate_menu": true, "number_sections": false, "sideBar": true, "threshold": "1", "toc_cell": false, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 0}