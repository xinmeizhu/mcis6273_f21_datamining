{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# MCIS6273 Data Mining (Prof. Maull) / Fall 2021 / HW2b\n", "\n", "**This assignment is worth up to 15 POINTS to your grade total if you complete it on time.**\n", "\n", "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n", "|:---------------:|:--------:|:---------------:|\n", "| 15 | Sunday, November 7 @ Midnight | _up to_ 20 hours |\n", "\n", "\n", "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n", "\n", "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n", "\n", "## OBJECTIVES\n", "* Continue practicing exploratory data analysis and visualization\n", "\n", "* Perform a clustering analysis using k-means\n", "\n", "## WHAT TO TURN IN\n", "You are being encouraged to turn the assignment in using the provided\n", "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n", "`homework/hw2b`.   Put all of your files in that directory. \n", "\n", "Then zip that directory,\n", "rename it with your name as the first part of the filename (e.g. `maull_hw2b_files.tar.gz`), then\n", "download it to your local machine, then upload the `.tar.gz` to Blackboard.\n", "\n", "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n", "on the basics of using in Linux.\n", "\n", "If you choose not to use the provided notebook, you will still need to turn in a\n", "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n", "this homework.\n", "\n", "\n", "## ASSIGNMENT TASKS\n", "### (25%) Continue practicing exploratory data analysis and visualization \n", "\n", "In the last HW we explore some of the basic features of Pandas with graphic and data selection. This time we're going to go a but deeper into Pandas ans learn about MultIndices and grouping data in interesting and useful ways.\n", "\n", "Power weightlifting (powerlifting) is an international sport that invites advanced amateurs and professionals alike. Fortunately, there are datasets for the multitude of powerlifting competitions around the world, and they are openly available for curious data scientists like ourselves who would like to ask interesting questions and find interesting relationships in the data. Whether you're into the sport or not, I think there are a variety of interesting phenomenon in the data that make it both tractable and interesting from just a data perspective.\n", "\n", "**DATA**\n", "\n", "[OpenPowerLifting.org](https://openpowerlifting.org) is a large set of data for a multitude of data related to powerlifting competitions around the world. The core data live at the following open source repository on [gitlab.com/openpowerlifting/opl-data](https://gitlab.com/openpowerlifting/opl-data).\n", "\n", "One of the things that we will learn from the data is that the majority of it are\n", "interesting over several dimensions.  There are the years of competition, the sex of the competitors,\n", "the age the competitors, country of origin, among other things.  With denser data like these, we want\n", "to understand some of the underlying groupings for easier access to the data.  For example,\n", "one might want to understand how groupings by year and age bear out on the data to explore questions\n", "like \"Has the number of competitors over 40 increased over the years?\"  This might be an interesting\n", "question to ask to explore if powerlifters continue to compete as they age since the sport is very\n", "difficult on one's body and requires intense continuous training to stay competitive.\n", "\n", "Some questions like these are also very useful to explore visually, so we'll dive into a few more\n", "graphical techniques to get at these answers and more.  We're going to end up with a DataFrame\n", "that will group our data by year, age class and sex, so we can see some of the interesting\n", "annual trends along each of these dimensions within the last two decades.\n", "\n", "&#167;  **BUILD THE DATASET**\n", "\n", "We've learned CSV is common file format for data and we will be working the files large text files in Gitlab to do the work we need. The task is to explore the repository and build up a dataset of 15 random lifting meets from 2019 using BeautifulSoup and the tools in Pandas to put these datasets together.\n", "\n", "Oridinarily, we would use a technique often known as \"crawling\" and is consider by some to be a flagrant violation of good web etiquette. However, the technique is still often the only way to obtain data en masse from a single source. If this were an FTP server, the same pattern could be applied and would not be considered unusual to do so. Of course, you must use this with caution, as it can result in IP throttling and IP blocking, so please use it within the licensing terms of both the data and website you are obtaining data from. Good web citizens restore trust in providers and administrators alike, so throttling yourself after your own requests with code like time.sleep(2) (which will pause your code for 2 seconds), will show that you can behave responsibly.\n", "\n", "Because we have to employ specific techniques when crawling\n", "datasets from dynamic web pages made with Javascript, and we're in a good position\n", "to just get the data we need from the ZIP file in the repository,\n", "we'll just assume we have already downloaded it and\n", "use the data provided in the `data/` folder which is just a\n", "random subset of 40 folders from the repository.\n", "\n", "You will take the random data and build yet another dataset\n", "of just the data from 2019.  See the supplemental notebook\n", "to see how to do this, but the easiest is to use the\n", "[Python `glob` module](https://docs.python.org/3/library/glob.html).\n", "\n", "**Load all the 2019 datasets into a single Pandas DataFrame**.\n", "\n", "Your DataFrame should have around 21K rows.\n", "\n", "\n", "&#167;  **FILTER AND EXPLORE THE DATA**\n", "\n", "Let's first get a feel for the data and filter it down.  One of the main\n", "difficulties in dealing with large user-contributed data sets like these\n", "are _data consistency_ and _data quality_.  _Data consistency_ refers to\n", "how data is represented over time.  We can see how this becomes an issue\n", "when we look at the `Division` column of the dataset.  We can see with a\n", "relatively untrained eye to the data, that something is very wrong with\n", "the consistency &mdash; there are nearly 400 Division designations!  \n", "\n", "When look at it more closely, there are groupings that overlap.  For\n", "example, you will see `Masters 45-49` and `Masters 40-49` when you\n", "perform a `.value_counts()` on the `Divisions` column of the data (see\n", "supplemental notebook).  What is the difference between these two since\n", "they obviously overlap?  Coming from the outside, we might not easily answer\n", "that question.\n", "\n", "We'll perform a few exploratory exercises to get a feel for the data.\n", "\n", "Answer the following questions in your notebook:\n", "\n", "1. What are the densities (counts) of participants in the top 5 `Division`s?\n", "1. How many `M` and `F` sex values are in the dataset?\n", "1. After dropping `NaN` values, what are the top 5 countries in the data, by number of datapoints?\n", "1. For the country of _Belarus_, how many participants are there in this data?\n", "1. Out all the data, how much of it is missing all of `Age`, `BirthYear` and `BirthDate`?  Give both the raw number and the percentage.\n", "\n", "\n", "&#167;  **CLEAN THE DATA**\n", "\n", "From the previous section, you learned that a good percentage of the data is lacking age\n", "information.  You also learned that between `Age`, `BirthDate`, and `BirthYear`, there\n", "are quite a few gaps, but that the age coverage could be increased if we took the time\n", "to do so. \n", "\n", "We are going to fill in those gaps the easy way -- by making sure that `Age` has the data\n", "it needs as we are going to do analysis over that in the subsequent parts of the \n", "assignment (and it is one of the fields we can actually do something about).\n", "\n", "In this part, you will write a function and use the `apply()` method in Pandas\n", "to fill in the missing data.\n", "\n", "You will need to:\n", "\n", "* find all data missing `Age`, but has at least `BirthYear` and `BirthDate`.\n", "* calculate age by either subtracting `BirthYear` from 2019 **or** by subtracting the \n", "  year part from `BirthDate`.\n", "* in your final DataFrame, make sure Age is an `int` type.\n", "* show all the steps in your notebook to get full credit.\n", "\n", "After doing that answer the following questions:\n", "\n", "1. How many new datapoints did you add to the dataset after filling in the missing Age values?\n", "\n", "\n", "&#167;  **GROUP THE DATA**\n", "\n", "Pandas provides superior capabilities to slice and group data.  We would like\n", "to build answer some questions of the 2019 data.\n", "\n", "You will need to study the following resources to complete the questions in this section:\n", "\n", "*  [`Dataframe.query()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html#pandas.DataFrame.query)\n", "*  [`DataFrame.groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby).  A useful hint when using this function is that you can pass to the first parameter of `DataFrame.groupby()` the list of the grouping in order of grouping, outer group first.  Thus, `groupby(['Sex', 'Division`]) will return the MultiIndex DataFrame with `Sex` as the outermost group\n", "and `Division` the inner group. \n", "\n", "**NOTE**: you will use the filtered data from the previous section -- you do not want to include the missing\n", "`Age` data in your analysis\n", "\n", "Answer the following question for full credit on this part:\n", "\n", "1. How many `M` sex participants are there in the `Open Pro` Division?\n", "1. What is the median age of all participants in the `Open` Division?\n", "1. What is the mean `BodyweightKg` in the `Open` Division men (`M`)?  \n", "1. How does this compare with `Open` Division women (`F`)?\n", "1. What is the correlation coefficient (Pearsons $r$ is fine) between `Best3BenchKg`?  Would you consider\n", "   this to be a positive, negative or no correlation?\n", "\n", "\n", "&#167;  **VISUALIZE**\n", "\n", "Now that we have the data segmented the way we'd like, let's visualize it\n", "in some interesting way.\n", "\n", "With powerlifting there are a number of ways to express the _strength_ of a\n", "competitor. There is _raw_ strength, meaning how much total weight was lifted\n", "on a given lift, and there is _relative_ strength.  It is not fair to compare\n", "the raw lift of a 100lb 16 year old teenage to that of a 35 year old 300lb adult.\n", "\n", "The 35 year old might lift ten times the weight yet the 16 year old may be _relatively_\n", "stronger, but how would we compare their\n", "_relative_ strengths?  Many competitors will be able to lift between 2 and 7 times\n", "their body weight depending on the lift, so we might expect a 100lb powerlifter\n", "to perhaps perform a 200lb bench press and maybe a 300lb squat, both impressive\n", "for their weight.  To deal with comparing _strength_ across age and weight variables\n", "a number of methods have been developed to create fair and accurate measures of\n", "_relative strength_.  The OpenPowelifting dataset includes three such measures:\n", "_Wilks_, _McCulloch_ and _Glossbrenner_, which give a numeric assignment of relative\n", "strength which factor age and weight into the computation.  Exploring the details\n", "of each of these methods is beyond the scope of this homework, but the curious can\n", "learn more on the variety of sites which calculate these statistics.\n", "\n", "We will restrict our interest to the _Glossbrenner_ score, which takes into account\n", "age and weight to compute a normalized weight value.  Consider three competitors,\n", "all 29 year olds, with one male and one female weighing 131.84lbs and the last male\n", "weighing 263.67 pounds.  Assume they all lift 639.33 pounds total.  The _Glossbrenner_ score\n", "takes into account the age and weights and produces a relative score with the\n", "following:\n", "\n", "| competitor | sex | age (lbs) | weight (lbs) | lift (lbs) | score | $\\gamma$-coefficient | $\\gamma_{age}$-coefficient\n", "|-----------:|:---:|:---:|:------:|:-----:|:-----:|:--------------------:|:----:|\n", "| 1 | F | 29 | 100 | 639.33 | 457.28 | 0.7152 | 1.0 |\n", "| 2 | M | 29 | 100 | 639.33 | 373.94 | 0.5849 | 1.0 |\n", "| 3 | M | 29 | 100 | 639.33 | 373.94 | 0.5849 | 1.0 |\n", "| 4 | M | 49 | 100 | 639.33 | 416.20 | 0.5849 | 1.113 |\n", "| 5 | F | 49 | 100 | 639.33 | 508.95 | 0.7152 | 1.113 |\n", "\n", "\n", "The _Glossbrenner_ score is in the _score_ column and the $\\gamma$-coefficient is\n", "the constant calculated by the method.  The $\\gamma_{age}$-coefficient is a constant which the \n", "method factors in for the relative impact age has on the\n", "competitor.  Thus, the _Glossbrenner_ score $\\Gamma$ is:\n", "\n", "$$\n", "\\Gamma(age, sex, weight) = \\gamma{\\text{-coefficient}}_{weight,sex} \\times \\gamma_{age} \\times weight\n", "$$\n", "\n", "You will need to use the code in the OpenPoweLifting repository to complete the calculation.\n", "\n", "You can download the [`coefficient.py`](https://gitlab.com/openpowerlifting/opl-data/-/raw/main/scripts/coefficient.py\\?inline\\=false) file to the directory where your notebook lives.  Then you can simply\n", "`import coefficient` and use it like in this example:\n", "\n", "```python\n", "\n", "import coefficient\n", "\n", "for i, d in enumerate(data):\n", "    if d[0]:\n", "        g_coeff = coefficient.glossCoeffMen(d[2])  \n", "    else:\n", "        g_coeff = coefficient.glossCoeffWomen(d[2])\n", "        \n", "    gb_score = \\\n", "        g_coeff  *\\\n", "        coefficient.AGE_COEFFICIENTS[d[1]] *\\\n", "        d[3]\n", "```\n", "\n", "Now that we have that out of the way, let's visualize some data.  Specifically, we'd like to\n", "plot the _Glossbrenner_ score for the last 20 years over time.  Are the scores going up, down\n", "or staying the same?  One could expect any of these scenarios to occur, so let's dive in.\n", "\n", "What we want to produce are two _area_ plots of the annual _mean Glossbrenner_ score\n", "from 1999 to 2018 for all age groups, one plot for males and the other for females as putting them all on\n", "one graph would most certainly be information overload.  To do this we will need to\n", "slice the data in a way that makes a multi-index grouped by year, age group and sex.\n", "\n", "You will  the data in `data/1999_to_2018` to do this and make a visual of it.\n", "\n", "Your area plot code will be invoked by:\n", "\n", "```python\n", "\n", "DataFrame.plot.area()\n", "\n", "```\n", "\n", "You may optionally pass in the `figsize=(15,7)` (or whatever dimensions you'd like)\n", "to stretch the data out a bit so you can visually see what is going on, since the\n", "legend may get in the way of viewing the data.\n", "\n", "Your plot will look something like this:\n", "\n", "![](./sample_area.png)\n", "\n", "Please see the [`DataFrame.plot.area()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.area.html#pandas.DataFrame.plot.area) method for full information on\n", "the area plots.\n", "\n", "**A final important note**: You will need to use [`droplevel()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.droplevel.html) and\n", "[`unstack()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unstack.html)\n", "in order to prepare your DataFrame for final presentation.  Basically,\n", "you'll need to drop the `Sex` level of your index (level 2) and immediately\n", "before you plot the area plot you will use `unstack()`.\n", "\n", "Your notebook must show:\n", "\n", "* the _Glossbrenner_ score in a new column that implements the calculation for the parameters \n", "  required.  You can drop all other columns if you like.\n", "* an area plot showing the male data grouped by age and year, that\n", "  is the $x$-axis will show the year and the $y$-axis the _Glossbrenner_ score,\n", "* an area plot showing the female data grouped by age and year, that\n", "  is the $x$-axis will show the year and the $y$-axis the _Glossbrenner_ score.\n", "\n", "You must answer the following question directly in your notebook:\n", "\n", "1. What's the general trend you see in the area plots?  Your answer can\n", "   be in one or two sentences.\n", "\n", "\n", "\n", "### (50%) Perform a clustering analysis using k-means \n", "\n", "The simplicity and power of k-means algorithm makes it one of the best to start with\n", "when performing _unsupervised learning_ &mdash; that is the class labels of your\n", "data are not known _a priori_ and you that will not be training the algorithm\n", "on labeled data.  While this is a powerful and oft useful technique, use it with\n", "care as the initial conditions of the algorithm do not guarantee a global maximum\n", "and as such, running the algorithm with a number of initialization points will\n", "produce better and more reliable results.\n", "\n", "Continuing with our OpenPowerlifting data, we're going to do some exploratory data\n", "analysis to examine this dataset in some interesting ways using unsupervised learning,\n", "namely clustering.  The original dataset has over 1 million data points, but in order\n", "to get a good idea of what's in it, we will not need to go back through the entire\n", "dataset, and in fact, we will restrict the focus of our energy on just the last 2 decades\n", "from 1999.\n", "\n", "**REMEMBER TO MAKE SURE TO SHOW ALL YOUR WORK IN THE NOTEBOOK SO YOU CAN RECEIVE PARTIAL CREDIT WHERE APPROPRIATE!**\n", "\n", "&#167;  **PREPARE FOR CLUSTERING**\n", "\n", "You will need to complete part 1 of this homework to filter the data to the necessary\n", "subset for this part.  The subset of features will just\n", "be the following:\n", "\n", "  ```python\n", "  features = [\n", "      'Sex',\n", "      'Age',\n", "      'BodyweightKg',\n", "      'Best3SquatKg',\n", "      'Best3BenchKg',\n", "      'Best3DeadliftKg',\n", "      'TotalKg'\n", "    ]\n", "  ```\n", "\n", "Final preparation for clustering will require you to turn all of\n", "_categorical_ variables into _numeric_ one's.  One way to do this\n", "from directly within Pandas is to use `Pandas.get_dummies(your_dataframe)`.\n", "You can also study the\n", "`sklearn.preprocessing.OrdinalEncoder()` which will do something\n", "very similar.  Either way, with the reduced set of features above, the only\n", "categorical variable will be `Sex` as all the others should already\n", "be numerical features.\n", "\n", "In your notebook, you should show:\n", "\n", "* clearly how many features are now in your dataframe?\n", "* that you are using the **cleaned** data on `Age`\n", "* you should also remove any rows where `NaN` is in any of required columns (hint: `dropna()`)\n", "\n", "\n", "&#167;  **PERFORM SILHOUETTE ANALYSIS**\n", "\n", "In class we talked about the fact that the $k$ number of clusters needs to be\n", "determined _a priori_ &mdash; that is you will need to know how many clusters beforehand to\n", "run the algorithm.  To find the optimal $k$, we will use a method called the _silhouette score_.\n", "\n", "Adapt the following code to compute the silhouette scores on *only* the dataset filtered by\n", "the features from the prior step.\n", "\n", "```python\n", "  from sklearn.cluster import KMeans\n", "  from sklearn.metrics import silhouette_score\n", "\n", "  Sum_of_squared_distances = []\n", "  K = range(2, 15)\n", "  for k in K:\n", "      km = KMeans(n_clusters=k, n_init=20)\n", "      km = km.fit(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS)\n", "      Sum_of_squared_distances.append(km.inertia_)\n", "\n", "      silh_score = silhouette_score(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS, km.labels_)\n", "      print(\"k = {} | silhouette_score = {}\".format(k, silh_score))\n", "```\n", "\n", "The largest score is typically the $k$ you go with.  If $k=2$ is your largest\n", "score, we will ignore and use the next best score since 2 clusters is not usually an\n", "interesting number of clusters when dealing with a large set of data points.\n", "\n", "Your notebook must show and answer the following:\n", "\n", "1. What is the optimal $k$ according the silhouette score?\n", "1. What else is interesting about the scores?\n", "\n", "\n", "&#167;  **CLUSTER INTERPRETATION**\n", "\n", "Now that you have clusters and optimal cluster, let's find out the characteristics of\n", "the features that dominate them.\n", "\n", "Note that the k-means algorithm returns the cluster centers\n", "for each cluster, hence in that center each feature value\n", "is the _representative feature value_ for that cluster.\n", "For example, the `TotalKg` would be the representative `TotalKg` for\n", "that cluster.\n", "\n", "Using the optimal cluster size from the silhouette score in the prior\n", "section, please use adapt the following code to determine the cluster\n", "characteristics.\n", "\n", "```python\n", "    optimal_k = THE_OPTIMAL_SILH_K\n", "\n", "    km = KMeans(n_clusters=optimal_k, n_init=150)\n", "    km = km.fit(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS)\n", "\n", "    for i in range(0, optimal_k):\n", "        l = list(zip(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS.columns, \\\n", "                    km.cluster_centers_[i]))\n", "        l.sort(key=lambda x: x[1], reverse=True)\n", "\n", "        print('CLUSTER : {}\\n'.format(i))\n", "        for attr, val in l[:]:\n", "          print('\\t{} : {}\\n'.format(attr, val))\n", "```\n", "\n", "Your notebook must show and answer the following:\n", "\n", "1. for each cluster, describe in real words what the cluster centers are telling\n", "  you about the representative of that cluster.  For example, your answer might\n", "  look like: \"for cluster 1, the representative for that cluster is a 24.7 year\n", "  old female, with an average `Best3SquatKg` of 121 and a `TotalKg` of 721\",\n", "1. show the output of the cluster centers above.\n", "\n", "**NOTE**: The order of the features in `km.cluster_centers_` are the same order\n", "as they exist in the DataFrame.\n", "\n", "\n", "\n"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [default]", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "12px", "width": "252px"}, "navigate_menu": true, "number_sections": false, "sideBar": true, "threshold": "1", "toc_cell": false, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 0}